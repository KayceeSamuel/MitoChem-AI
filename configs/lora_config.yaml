model_name: Qwen/Qwen2.5-7B-Instruct

lora_params:
  r: 16 # LoRA attention dimension
  lora_alpha: 32 # Scaling factor for LoRA weights
  lora_dropout: 0.05 # Dropout probability
  bias: "none" # Bias type
  task_type: "CAUSAL_LM" # Task type
  # Target modules for Qwen2.5 (all linear layers)
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training_params:
  num_train_epochs: 1
  learning_rate: 1e-5 # CRITICAL: Low learning rate for stability
  max_grad_norm: 0.3 # CRITICAL: Gradient clipping for safety
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  optim: "paged_adamw_8bit"
  warmup_steps: 100
  logging_steps: 25
  save_steps: 250
